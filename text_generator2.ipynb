{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled75.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkUxP137ofl2"
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.utils import get_file\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XtvAJBB2o6N4",
        "outputId": "b817c29a-4aaa-448b-b15a-68f636b55d71"
      },
      "source": [
        "data1 = get_file(\"a_childhood_of_the_orient\", 'https://www.gutenberg.org/files/66019/66019-0.txt')\n",
        "data2 = get_file('MISS LOCHINVAR', 'https://www.gutenberg.org/files/66018/66018-0.txt')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.gutenberg.org/files/66018/66018-0.txt\n",
            "311296/304380 [==============================] - 0s 1us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-C8pl7wqL3T"
      },
      "source": [
        "text1 = open(data1, 'rb').read().decode(encoding = 'utf-8')\n",
        "text2 = open(data2, 'rb').read().decode(encoding = 'utf-8')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-a0I7F0qm8Z"
      },
      "source": [
        "text1 = text1[3000:]\n",
        "text2 = text2[3000:]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9SFmy11rP-6"
      },
      "source": [
        "text = text1 + text2"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iD_zzXVtrp1f",
        "outputId": "f6515f0b-8034-4d29-be02-86596db360d0"
      },
      "source": [
        "len(text)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "676936"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RONOrlzZrq4e",
        "outputId": "fb29020f-fba4-4ad1-99e3-b57e3163e2d5"
      },
      "source": [
        "print(text[:250])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "is_, you are five years old. I wish you many\r\n",
            "happy returns of the day.”\r\n",
            "\r\n",
            "He drew up a chair, and sat down by my bed. Carefully unfolding a piece\r\n",
            "of paper, he brought forth a small Greek flag.\r\n",
            "\r\n",
            "“Do you know what this is?”\r\n",
            "\r\n",
            "I nodded.\r\n",
            "\r\n",
            "“Do you\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgX_4q3Irxb0"
      },
      "source": [
        "vocab = sorted(set(text))\n",
        "\n",
        "char2idx = {u : i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNS4Mso_sKbU",
        "outputId": "7e15de31-ce07-41da-c011-9e9a0f3ab269"
      },
      "source": [
        "for char, _ in zip(char2idx, range(20)):\n",
        "  print(char, char2idx[char])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " 0\n",
            "\r 1\n",
            "  2\n",
            "! 3\n",
            "\" 4\n",
            "$ 5\n",
            "% 6\n",
            "& 7\n",
            "' 8\n",
            "( 9\n",
            ") 10\n",
            "* 11\n",
            ", 12\n",
            "- 13\n",
            ". 14\n",
            "/ 15\n",
            "0 16\n",
            "1 17\n",
            "2 18\n",
            "3 19\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Efi0wakfr_a8",
        "outputId": "1dfaf51b-4f5e-492e-b985-66439b8e7bc5"
      },
      "source": [
        "text_as_int = [char2idx[c] for c in text]\n",
        "text_as_int[:5]"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[66, 76, 57, 12, 2]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vb11NQxLsIV2",
        "outputId": "5e3c3b18-697f-45c9-a34c-88bd20dd776f"
      },
      "source": [
        "seq_len = 100\n",
        "examples_per_epoch = len(text) // seq_len\n",
        "\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "for i in char_dataset.take(10):\n",
        "  print(idx2char[i])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "i\n",
            "s\n",
            "_\n",
            ",\n",
            " \n",
            "y\n",
            "o\n",
            "u\n",
            " \n",
            "a\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w4t93uHZszBg",
        "outputId": "12290279-6eab-4663-9b59-6e0eab6c5938"
      },
      "source": [
        "seq = char_dataset.batch(seq_len +1 , drop_remainder= True)\n",
        "\n",
        "for item in seq.take(5):\n",
        "  print(repr(''.join(idx2char[item.numpy()])))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'is_, you are five years old. I wish you many\\r\\nhappy returns of the day.”\\r\\n\\r\\nHe drew up a chair, and s'\n",
            "'at down by my bed. Carefully unfolding a piece\\r\\nof paper, he brought forth a small Greek flag.\\r\\n\\r\\n“Do'\n",
            "' you know what this is?”\\r\\n\\r\\nI nodded.\\r\\n\\r\\n“Do you know what it stands for?”\\r\\n\\r\\nBefore I could think of'\n",
            "' an adequate reply, he leaned toward me and said\\r\\nearnestly, his fiery black eyes holding mine:\\r\\n\\r\\n“I'\n",
            "'t stands for the highest civilization the world has ever known. It\\r\\nstands for Greece, who has taught'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1HTyK_OtJ22"
      },
      "source": [
        "def split_input_target(chunk):\n",
        "  input_text = chunk[:-1]\n",
        "  target_text = chunk[1:]\n",
        "  return input_text, target_text\n",
        "\n",
        "dataset = seq.map(split_input_target)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VS-OEdZWtyKw"
      },
      "source": [
        "batch_size = 64\n",
        "buffer_size = 10000\n",
        "\n",
        "dataset = dataset.shuffle(buffer_size).batch(batch_size, drop_remainder = True)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "148mxNzbuOwp"
      },
      "source": [
        "vocab_size = len(vocab)\n",
        "\n",
        "embedding_dim = 256\n",
        "\n",
        "rnn_units = 1024"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SyDrRs9uU7X"
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = Sequential([\n",
        "                      Embedding(vocab_size, embedding_dim, batch_input_shape = [batch_size, None]),\n",
        "                      LSTM(rnn_units, return_sequences= True, stateful= True, recurrent_initializer= 'glorot_uniform'),\n",
        "                      Dense(vocab_size)\n",
        "  ])\n",
        "  return model"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKQhwZsPu41R"
      },
      "source": [
        "model = build_model(vocab_size = vocab_size, embedding_dim= embedding_dim, rnn_units= rnn_units, batch_size = batch_size)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKqKSxVdvrEs",
        "outputId": "106391bc-af40-489c-8a10-4d72873238bc"
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "  example_batch_prediction = model(input_example_batch)\n",
        "  print(example_batch_prediction.shape)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 100, 98)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JN09qk1WwIRE",
        "outputId": "1c6253a6-82fb-4c24-a97e-a5569250a317"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (64, None, 256)           25088     \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (64, None, 1024)          5246976   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (64, None, 98)            100450    \n",
            "=================================================================\n",
            "Total params: 5,372,514\n",
            "Trainable params: 5,372,514\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZE_-gluwbF9"
      },
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_prediction[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis = -1).numpy()"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Js-18g8dwrr6",
        "outputId": "dd4da05d-f4f0-4fce-d8fe-2c1ab282698f"
      },
      "source": [
        "print(sampled_indices)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 4 43 23 11 24 18 69  3 55 12 35 86 43 12 18 37 84 22 16 43 52 95 74 60\n",
            " 12 46 88 38 68 82 57  8 35 12 33  8 56 39 37 67 51 74 40 33 45 71 78 77\n",
            " 68 48 65 52 81 27 33  0 91  1 53 13 66 24 69 34  6 71 21 12 93 83 85 11\n",
            " 61 95 30 89 38 16 43 86  4  5 54 76 70 69 38  9 93 65 22 64  3 22 12 47\n",
            "  9 28 43 23]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6QiZU0XgwwNw",
        "outputId": "75afbe65-be33-4012-a335-8bdb8b005cc0"
      },
      "source": [
        "print('input:',repr(''.join(idx2char[input_example_batch[0]])))\n",
        "print('expected :', repr(''.join(idx2char[sampled_indices])))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input: 'e\\r\\n_parti_ if he had money and position, irrespective of any other\\r\\nqualifications.\\r\\n\\r\\nFor a long ti'\n",
            "expected : '\"O7*82l![,GÏO,2IÉ60OX’qc,RæJky_\\'G,E\\']KIjWqLEQnutkThXx;E\\në\\rY-i8lF%n5,ôzË*d’BèJ0OÏ\"$ZsmlJ(ôh6g!6,S(?O7'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5GFh9gyxOjX"
      },
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits = True)\n",
        "\n",
        "example_batch_loss = loss(target_example_batch, example_batch_prediction)\n"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3m1QGuOxh5-"
      },
      "source": [
        "model.compile(optimizer = 'adam', loss = loss)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sx_YuBQCxxrO"
      },
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt_ {epoch}')\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(filepath= checkpoint_prefix, save_weights_only= True)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pu0VT5-byLIv",
        "outputId": "f16c7b25-17c0-408a-aa38-156d68223177"
      },
      "source": [
        "epochs = 20\n",
        "\n",
        "history = model.fit(dataset, epochs = epochs, callbacks=[checkpoint_callback])"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "104/104 [==============================] - 10s 69ms/step - loss: 2.8178\n",
            "Epoch 2/20\n",
            "104/104 [==============================] - 8s 70ms/step - loss: 2.1709\n",
            "Epoch 3/20\n",
            "104/104 [==============================] - 8s 72ms/step - loss: 1.8932\n",
            "Epoch 4/20\n",
            "104/104 [==============================] - 8s 73ms/step - loss: 1.7058\n",
            "Epoch 5/20\n",
            "104/104 [==============================] - 8s 73ms/step - loss: 1.5750\n",
            "Epoch 6/20\n",
            "104/104 [==============================] - 8s 72ms/step - loss: 1.4832\n",
            "Epoch 7/20\n",
            "104/104 [==============================] - 8s 71ms/step - loss: 1.4122\n",
            "Epoch 8/20\n",
            "104/104 [==============================] - 8s 71ms/step - loss: 1.3576\n",
            "Epoch 9/20\n",
            "104/104 [==============================] - 8s 70ms/step - loss: 1.3101\n",
            "Epoch 10/20\n",
            "104/104 [==============================] - 8s 70ms/step - loss: 1.2681\n",
            "Epoch 11/20\n",
            "104/104 [==============================] - 8s 71ms/step - loss: 1.2301\n",
            "Epoch 12/20\n",
            "104/104 [==============================] - 8s 71ms/step - loss: 1.1933\n",
            "Epoch 13/20\n",
            "104/104 [==============================] - 8s 72ms/step - loss: 1.1573\n",
            "Epoch 14/20\n",
            "104/104 [==============================] - 8s 72ms/step - loss: 1.1222\n",
            "Epoch 15/20\n",
            "104/104 [==============================] - 8s 72ms/step - loss: 1.0856\n",
            "Epoch 16/20\n",
            "104/104 [==============================] - 8s 71ms/step - loss: 1.0478\n",
            "Epoch 17/20\n",
            "104/104 [==============================] - 8s 70ms/step - loss: 1.0104\n",
            "Epoch 18/20\n",
            "104/104 [==============================] - 8s 70ms/step - loss: 0.9698\n",
            "Epoch 19/20\n",
            "104/104 [==============================] - 8s 71ms/step - loss: 0.9314\n",
            "Epoch 20/20\n",
            "104/104 [==============================] - 8s 71ms/step - loss: 0.8900\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lFrsW3W6yTEv",
        "outputId": "69ce1772-7ed1-400c-bcc5-a0845106f25f"
      },
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size = 1)\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "model.build(tf.TensorShape([1, None]))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (1, None, 256)            25088     \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (1, None, 1024)           5246976   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (1, None, 98)             100450    \n",
            "=================================================================\n",
            "Total params: 5,372,514\n",
            "Trainable params: 5,372,514\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSwhCzWEzCAO"
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "  num_generate = 10000\n",
        "\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  text_generated = []\n",
        "\n",
        "  temperature = 1.0\n",
        "\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "    predictions = model(input_eval)\n",
        "    predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "    predictions = predictions / temperature\n",
        "    predicted_id = tf.random.categorical(predictions, num_samples = 1)[-1, 0].numpy()\n",
        "\n",
        "    input_eval = tf.expand_dims([predicted_id], 0)\n",
        "    text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BChgaWuP0KK5",
        "outputId": "dc688ac0-155a-4497-ad9f-ddb077fde016"
      },
      "source": [
        "print(generate_text(model, start_string= u'The youngest member'))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The youngest members of Poop. Bago\r\n",
            "I would not be mostly and fashion, and the day\r\n",
            "\r\n",
            "It is the blue forestors they were cervanted years age my like children of\r\n",
            "Arif But the Hummel the other ways was laigh\r\n",
            "a mindleady predition. “Nou indite her going touch\r\n",
            "your true.”\r\n",
            "\r\n",
            "“But within be present,” I commored him. “Fork meware her\r\n",
            "fact that the school and behered.\r\n",
            "At the epists the man of the first time free\r\n",
            "days. In my heart was not for the children had been moreatest friends, and then\r\n",
            "she was struckly closed, and then buy had phints treath\r\n",
            "of membaring shone mitherly applicable teever myself for\r\n",
            "her trousper. She esurply, the untrain, the heroine\r\n",
            "experience she could not each _chedians. I struck quite failing a\r\n",
            "numsur’ at Themenuen understand the brown sided was covered with\r\n",
            "afternative, and--and I do not see much all offend me.”\r\n",
            "\r\n",
            "“Hang loving!”\r\n",
            "\r\n",
            "“I wonder like about to-say opened.”\r\n",
            "\r\n",
            "“When what to a ntile, if you will be ablulged. Of the\r\n",
            "inCense\r\n",
            "that I had been told that that he asked time up the valiars feor\r\n",
            "attendants breakfasting as they are assidants as life in relief?\r\n",
            "\r\n",
            "[that was the second time a linand abouteful man, I could\r\n",
            "have rese.\r\n",
            "\r\n",
            "“After I heppiness you are right. You at\r\n",
            "once my flished befire, holding many more first person to paps at ouch.\r\n",
            "\r\n",
            "Again I threw hers on its close law I created\r\n",
            "vaintle with this I would call them.”\r\n",
            "\r\n",
            "From Summon Kallering it bent, and to let him te lighted her cousin. Jan\r\n",
            "gave for a ten, that my cousin began to have her every day more than any one to kills and kissed me. It was neither\r\n",
            "then. When the greatest parts is discovered she was wonderful the indifference to the day of the uppar which enthusiasts\r\n",
            "for a fit of thinks before her slaves required to each other he\r\n",
            "to permit tigsting in that man, “why did you see ready feel close or attinent, and its is the\r\n",
            "girl wom which Jan had consulted her heart, and\r\n",
            "when the rooms are to buy part of getting morification\r\n",
            "number or currental way the iprogrchap at Ali Baba. It active. Betray,\r\n",
            "like Daisy was banister without even longer,” said\r\n",
            "Jan, who plan for centerness, far away from since, Jan threw outher\r\n",
            "games to bul the landing, he was a garling for accidance in life-smile. She had not\r\n",
            "a city assame at her. The storm has left beyond him. Besides,\r\n",
            "they were asked whether I could have made her he was her into the villitary post in her efforts there. It was from a sudden’s dog By a\r\n",
            "Pasha, and very little Djimlah only he began some committy becimed\r\n",
            "from Gwen’s head. In a square arment dather to bithers as a\r\n",
            "sigh of working. My book telling me even into life perfect\r\n",
            "guest.\r\n",
            "\r\n",
            "A fortunate animaldaraties.\r\n",
            "\r\n",
            "[Illustration: The monk had always had heard after\r\n",
            "this was frughted about them ever since the Greek revisence, who, lady had\r\n",
            "just harded faver a changes. In the other dawn the\r\n",
            "boards with her at the house alone--it nice at the and on that could be her hair.\r\n",
            "\r\n",
            "“No, I know.”\r\n",
            "\r\n",
            "“If you lent you this--me did ever those Turkish pobrors\r\n",
            "of the world.”\r\n",
            "\r\n",
            "“Gone beyove,” and as Jan ppantly, studied and peaches with reparation for\r\n",
            "superstitions to eat might. But though the books now\r\n",
            "stroke, and this was the class was the soon, but nonsineeally, though there threw off them glad to\r\n",
            "celebrate you!”\r\n",
            "\r\n",
            "“Until y’s heeld, mamma\r\n",
            "use cause. And French lained was perhaps\r\n",
            "at its we denerally absolutely, nor did he turned to the\r\n",
            "Jan’s hour before I had before I could bore or history, ite\r\n",
            "told at reason, the faces and tame to the paper for\r\n",
            "cermining better than both papare we hidden leaving being laderstases, in paragraph 1.E below. “Fract\r\n",
            "Hemethey’ halled the Turks. They had their live,” said Jan, since\r\n",
            "in a man’s-line. The little time in its lone forms aworemones. My intimat\r\n",
            "patriotism at that moment we were at a simple with have y\r\n",
            "come.\r\n",
            "\r\n",
            "“Come! ‘THE \r\n",
            "THE ASUSS.\"\r\n",
            "\r\n",
            "\r\n",
            "After A long aside without the books of the effendi, reminding\r\n",
            "them, and in and out of those who could be nice.”\r\n",
            "\r\n",
            "Janet ready to each other nicer than perfectly insilement and her forehead even uncillion as Gladys, having badly we made this purposes pure. Ih all her\r\n",
            "going to reach our nise corner.\r\n",
            "\r\n",
            "Apper to Jan should think, and I tried to be in her even frung farther for me. I was ready to\r\n",
            "have the dim Turks, because she was a little dumble feet, and then Can lets themselves nursery that there was\r\n",
            "so! I meant used to leave Turkey. I could pass meant me. He wished my pass\r\n",
            "was brough with an left the given engult to remember that one around herself to meen my own different forms of which Djimlah and Cena\r\n",
            "Susantly she apped my animal--past her bandage into herself, and were struck\r\n",
            "Jan, who, and we had plensed for an ensity that I should come back again. This it\r\n",
            "was unconscious of attitude toward her.\r\n",
            "\r\n",
            "At that night I only nodded to them.\r\n",
            "\r\n",
            "“Why did you do?” she’d past armin as Kanti, who\r\n",
            "would perchaise them lapped us and lockeds--after all, a rich humalous North over\r\n",
            "More like a large beside.\r\n",
            "After this eBook (amountane long, for\r\n",
            "very about St. Norking or proprie assurance failed with intimate\r\n",
            "and makemanies. “I thinks about beying to rum\r\n",
            "I do not noble you averimentain thought of all seven orders. Sydney\r\n",
            "nelted stop in an unself. I do!” she asked, and at\r\n",
            "Pashatwatedner and Sitanthy\r\n",
            "salutedly. And that he time to your mother the\r\n",
            "production to have Gwen found herself to me.\r\n",
            "\r\n",
            "“What is what you permause she was a girl over the atmosphere. In a few moments he\r\n",
            "would begin amous Viva was the secced to\r\n",
            "her.”\r\n",
            "\r\n",
            "_Wangeriag_. Besides,” he conquered her heart ton, about them.\r\n",
            "It was at the nd found in in her returned form anynotseip mither.\r\n",
            "For is the only snot one sittic, and were found in that time I had for\r\n",
            "swell beautiful and an inside, are so much man, and as I asked\r\n",
            "her cousin Janet. I wished to miss the foreible hour of a little-ticken knew of it, even\r\n",
            "if that we stupend of wore here, and told us at the back riddle coling, and the\r\n",
            "paper. This escended iname up to\r\n",
            "since the\r\n",
            "first thing. I might have had managed to send to him Jan\r\n",
            "to play with Gwen, his head a remorch fram _pathab_.\r\n",
            "\r\n",
            "Home, homeloted considerable as he, they always warm\r\n",
            "affectation again denished merbed in the end was under from the darkness.\r\n",
            "The east wite Fred, I might have been holding that it may\r\n",
            "lead must me entirely as at this furship endicance\r\n",
            "in a brace of minutes niece, you do anything more than speaker, if I kept\r\n",
            "tintestled and make her headerly, Syd, that something for us things and\r\n",
            "have me the wind I’m learn to reprive on that darkness.\r\n",
            "\r\n",
            "He caughed she reassuced his father and kissed quickly\reg home, a dolla’s man as a vishit--entill have you here ways in much aller in than little\r\n",
            "finger-shoe for our pehiods. It made a very nightfal\r\n",
            "nest to hear ose profits to help long, while I had to accomplish nationam a bad in exlier. The\r\n",
            "arenity nor prite in checks in the spiders. It\r\n",
            "ensolate Indian for a persona who will ve pown of able to\r\n",
            "edrhanger, and many of them at all eusy\r\n",
            "ton manners through the atmosphere, of the piece of the\r\n",
            "subsline characce is in the station, during this work (all high\r\n",
            "for a minute from sending him half-by, inquiries for us to lose my feither. There we are Allah, the\r\n",
            "nobsessed in out of the room.\r\n",
            "\r\n",
            "From hands--iss.\r\n",
            "\r\n",
            "“Gwendoline, Jund, of bechieve, I thinks, because they had a\r\n",
            "sich girl, not becring mose confident the Graham faith in pungs that she answered nebroly\r\n",
            "recuried by reliefatively. Only Sydney and Mr. Graham forentener? I\r\n",
            "tasted.\r\n",
            "\r\n",
            "“To play you are?”\r\n",
            "\r\n",
            "“No, the music and sainture--as he was more there to Greeks. The baby of\r\n",
            "Gwen’s heart\r\n",
            "at the chocks. I rood only began to repart to never\r\n",
            "the little flag from be bods which age children if they could be able to say it. It always said nothing that\r\n",
            "Miss Lochinvar! To prace Turkish things are the manner. When\r\n",
            "she’ll have less possible.”\r\n",
            "\r\n",
            "Sydney heard. I gazed on the floor,\r\n",
            "and ashe\r\n",
            "parting acting, despitite the a dombination this\r\n",
            "leg.”\r\n",
            "\r\n",
            "She shoold horring them, one of her language in the dugies, to give you\r\n",
            "accident from her did so one afternating any beyind of the\r\n",
            "werthopges. Out of her should should hand\r\n",
            "from her and I could not any longer; then, to show how\r\n",
            "they had never consented--maddanderstand actual. On the saint of provided it was the production whom\r\n",
            "she had discussed to Arif and pressic undished.\r\n",
            "\r\n",
            "“Hall repress little fee that, better.”\r\n",
            "\r\n",
            "“G am hold Dride, with several respinsing emect Gutenberg-tm electronic works\r\n",
            "\r\n",
            "For anything so very so hard was sobt on the sleeping-class on the same time, Schublan, and he has\r\n",
            "preceded the events I\r\n",
            "bevery way unkeptafal past protested associated in accident holdesty-hall be stretce the door\r\n",
            "frest.\r\n",
            "\r\n",
            "It is a pertape,” said Gwen. “Good-time and not tell the extending of this grown-up stock I felt there, and\r\n",
            "my end is unpredumed, incrudiously rose, tracefully stronger, and her mother longed for and on, “the\r\n",
            "young half in such parangate him. or obtained my intream all for a week--you\r\n",
            "wouldn’t say your arrive Foundation is any one. I hear by a rush, where\r\n",
            "I sat down cooly I’d hair to come into the full Project Gutenberg-tm License must call half people are babies to did. I know it became more demiplose in a lot, from\r\n",
            "forth included. Thould take I wouldn’t try for equipment including Project Gutenberg. But at the monastery\r\n",
            "which the young fields were very little for luttle:\r\n",
            "“His poor girls fown about dew song\r\n",
            "thoughtfully. I was still deced them and baded the Greeks\r\n",
            "and year away, and I do ever seen anything but consuried in\r\n",
            "their leaun the pipsible demiliance with this race me.\r\n",
            "I after a long tine then, try that men anguisplessed the compose\r\n",
            "of streegs on her arm. She approached me better than point over, and her eyes\r\n",
            "his arms after they we’d have you back, and print call day from a land, next the glormed\r\n",
            "to match her and kissed his best.\r\n",
            "\r\n",
            "There was a nuisance at Jan quige fervently, since\r\n",
            "it may be and simple as bed at last a leas use. It was only \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLTmJCPe0SFI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}